# Libraries
import yaml
import time
import json
import shutil
import numpy as np
import pandas as pd

from pathlib import Path
from types import MethodType

# Specific libraries
from sklearn.metrics import make_scorer
#from sklearn.impute import SimpleImputer
#from sklearn.impute import IterativeImputer
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import Normalizer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
#from imblearn.over_sampling import SMOTE

# Own libraries
from ls2d.pipeline import PipelineMemory
#from ls2d.settings import _DEFAULT_FILTERS
from ls2d.settings import _DEFAULT_ESTIMATORS
#from ls2d.settings import _DEFAULT_PARAM_GRIDS
#from ls2d.settings import _DEFAULT_METRICS
#from ls2d.settings import _DEFAULT_TRANSFORMERS
from ls2d.utils import add_method


# --------------------------------------------------------
# Scoring functions
# --------------------------------------------------------
# This section can be adapted to each problem. In our
# secenario we compute the following metrics: loss,
# pearson, spearman, procrustes, shilouette, calinski,
# davies_b and gmm.

def custom_metrics(est, X, y):
    """This method computes the metrics.

    .. todo: Check whether the X value received here has
             already gone through previous steps such as
             inputation of missing data or preprocessing.
             The X value is is raw data in fit.

    .. todo: Check the outcomes of encoder, encode_inputs,
             transform, ... so that they are all consistent.

    .. note: The X and y represent the values passed to the
             .fit(X, y) method below. In general, y is either
             the class (classification) or a value (regression).

    .. note: Since we received the estimator, we need to apply
             the prediction/transformation ourselves manually
             as shown below.

    .. note: The scoring function must return numbers, thus
             including any string value such as the uuid or
             an autogenerated uuid4 does not work here.

    Parameters
    ----------
    est: object
        The estimator or pipeline.
    X:  np.array (or dataframe)
        The X data in fit.
    y: np.array (or dataframe)
        The y data in fit.
    """
    # Transform
    #y_embd = est.predict(X)
    y_embd = est.transform(X)
    # Metrics
    m = custom_metrics_(X, y_embd, y)
    # Compute loss
    try:
        loss = est.score(X, y_embd)
    except:
        loss = -1
    # Add information
    m['loss'] = loss
    # Return
    return m


def custom_metrics_(y_true, y_pred, y, n=1000):
    """This method computes the metrics.

    .. note: computation of pairwise distances takes too
             long when datasets are big. Thus, instead
             select a random number of examples and compute
             on them.

    Parameters
    ----------
    y_true: np.array (dataframe)
        Array with original data (X).
    y_pred: np.array
        Array with transformed data (y_emb).
    y: np.array (dataframe)
        Array with the outcomes
    n: int
        The number of samples to use for distance metrics.

    Returns
    -------
    dict-like
        Dictionary with the scores.
    """
    # Libraries
    from scipy.spatial.distance import cdist
    from scipy.spatial import procrustes
    from scipy.stats import spearmanr, pearsonr
    from sklearn.metrics import silhouette_score
    from sklearn.metrics import calinski_harabasz_score
    from sklearn.metrics import davies_bouldin_score
    from ls2d.metrics import gmm_scores
    from ls2d.metrics import gmm_ratio_score
    from ls2d.metrics import gmm_intersection_matrix
    from ls2d.metrics import gmm_intersection_area

    # Reduce computations which are expensive
    N = n if len(y_true) > n else len(y_true)
    idx = np.random.choice(np.arange(len(y_true)), N, replace=False)
    y_true_ = y_true.iloc[idx, :]
    y_pred_ = y_pred[idx]

    # Compute distances
    true_dist = cdist(y_true_, y_true_).flatten()
    pred_dist = cdist(y_pred_, y_pred_).flatten()

    # Compute scores
    pearson = pearsonr(true_dist, pred_dist)
    spearman = spearmanr(true_dist, pred_dist)

    # Compute procrustes
    try:
        mtx1, mtx2, disparity = procrustes(y_true, y_pred)
    except ValueError as e:
        mtx1, mtx2, disparity = None, None, -1

    # Compute scores for selected outcomes
    d_gmm = {}
    for c in y.columns:
        try:
            idx = y[c].notna()
            y_true_, y_pred_, y_ = \
                y_true[idx], y_pred[idx], y[c][idx]
            d_ = gmm_scores(y_pred_, y_)
            d_['silhouette'] = silhouette_score(y_pred_, y_)
            d_['calinski'] = calinski_harabasz_score(y_pred_, y_.ravel())
            d_['davies_b'] = davies_bouldin_score(y_pred_, y_.ravel())
            d_ = {'%s_%s'%(k, c) : v for k,v in d_.items()}
            d_gmm.update(d_)
        except Exception as e:
            print("Error: %s" % e)

    # Create dictioanry
    d = {}
    d['pearson'] = pearson[0]
    d['spearman'] = spearman[0]
    d['procrustes'] = disparity
    d.update(d_gmm)

    # Return
    return d


def predict(self, *args, **kwargs):
    return self.transform(*args, **kwargs)








if __name__ == '__main__':

    # ----------------------------------
    # Load arguments
    # ----------------------------------
    # Library
    import argparse

    # Default example (iris)
    DEFAULT_YAML = './datasets/iris/settings.iris.yaml'

    # Load
    parser = argparse.ArgumentParser()
    parser.add_argument("--yaml", type=str, nargs='?',
                        const=DEFAULT_YAML, default=DEFAULT_YAML,
                        help="yaml configuration file")
    args = parser.parse_args()


    # ----------------------------------
    # Set configuration
    # ----------------------------------
    # Library
    from ls2d.utils import AttrDict

    # Load configuration from file
    with open(Path(args.yaml)) as file:
        CONFIG = AttrDict(yaml.full_load(file))

    # Define pipeline path
    uuid = time.strftime("%Y%m%d-%H%M%S")
    workbench_path = Path(CONFIG.output) / uuid

    # The output path is create before because during the
    # training process the PipelineMemory class will be
    # storing in memory (pickle) the different pipelines.
    workbench_path.mkdir(parents=True, exist_ok=True)

    # Save settings file.
    shutil.copyfile(args.yaml, workbench_path / 'settings.yaml')



    # ----------------------------------
    # Load data
    # ----------------------------------
    # Load data from csv file.
    data = pd.read_csv(Path(CONFIG.filepath))

    # .. note: This lines is ensuring that only those observations
    #          which are complete (all features available) are used
    #          for training the models.
    data = data.dropna(how='any', subset=CONFIG.features)

    # Create X and y
    X = data[CONFIG.features]
    y = data[CONFIG.targets]

    # Compendium of results
    compendium = pd.DataFrame()

    # For each estimator
    for i, est in enumerate(set(CONFIG.estimators)):

        # Get the estimator
        estimator = _DEFAULT_ESTIMATORS[est]
        # Get the param grid
        param_grid = CONFIG.params.get(est, {})

        # Information
        print("\n    Method: %s. %s" % (i, estimator))

        # Option I: Not working.
        # Add the predict method if it does not have it.
        #estimator.predict = MethodType(predict, estimator)
        #estimator.predict = predict.__get__(estimator)
        #estimator.predict = predict

        """
        # .. note: The dimensionality reduction algorithms implemented
        #          in sklearn do have a transform method but not a
        #          predict method
        aux = getattr(estimator, "predict", None)
        if not callable(aux):

            # Option II:
            def predict(self, *args, **kwargs):
                return self.transform(*args, **kwargs)
            setattr(estimator.__class__, 'predict', predict)
        """""




        # .. note: It might be useful to include an inputer (either Simple
        #          or iterative) to fill missing values for future queries.
        #          Otherwise, these missing values need to be accounted for
        #          when making predictions.

        # Create pipeline
        pipe = PipelineMemory(steps=[
                                    #('simp', SimpleImputer()),
                                    #('iimp', IterativeImputer()),
                                    #('nrm', Normalizer()),
                                    #('std', StandardScaler()),
                                    ('minmax', MinMaxScaler()),
                                    (est, estimator)
                              ],
                              memory_path=workbench_path,
                              memory_mode='pickle',
                              verbose=True)

        # Warning
        if (workbench_path / pipe.slug_short).exists():
            print('[Warning] The pipeline <{0}> already exists... skipped!' \
                  .format(workbench_path / pipe.slug_short))
            continue

        # .. note: The whole dataset is used for training and for validation,
        #          this is all right in this scenario as we are looking for a
        #          compressed representation but cv should be changed for
        #          other types of problems (e.g. classification).

        # Create grid search (another option is RandomSearchCV)
        grid = GridSearchCV(pipe, param_grid=param_grid,
                                  cv=(((slice(None), slice(None)),)),
                                  scoring=custom_metrics,
                                  return_train_score=True, verbose=2,
                                  refit=False, n_jobs=1)

        # Fit grid search
        grid.fit(X, y)

        # Save results as csv
        results = grid.cv_results_

        # Add information
        df = pd.DataFrame(results)
        df.insert(0, 'estimator', _DEFAULT_ESTIMATORS[est].__class__.__name__)
        df.insert(1, 'slug_short', pipe.slug_short)
        df.insert(2, 'slug_long', pipe.slug_long)
        df['path'] = workbench_path / pipe.slug_short
        df.to_csv(workbench_path / pipe.slug_short / 'results.csv', index=False)

        # Append to total results
        compendium = compendium.append(df)

    # Save
    compendium.to_csv(workbench_path / 'results.csv', index=False)