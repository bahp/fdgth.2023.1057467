{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Create pipeline\n\nSample file to create a pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n\n    # Generic\n    import yaml\n    import time\n    import torch\n    import pickle\n    import pprint\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n\n    # Libraries\n    from pathlib import Path\n\n    # Own\n    from ls2d.utils import _load_pickle\n    from ls2d.utils import _dump_pickle\n\n    # ------------------\n    # Load config\n    # ------------------\n    # Configuration file\n    YAML_PATH = '../datasets/dengue/settings.dengue.yaml'\n\n    # Load configuration from file\n    with open(YAML_PATH) as file:\n        config = yaml.full_load(file)\n\n    # Variables\n    FEATURES = config['features']\n    DATAPATH = Path('..') / Path(config['filepath'])\n\n\n    # ------------------\n    # Load data\n    # ------------------\n    # Load data\n    data = pd.read_csv(DATAPATH)\n    data = data.dropna(how='any', subset=FEATURES)\n\n    # Test data\n    u1 = np.array([[1.,2.,3.,4.,5.]]) \\\n        .astype(np.float32)\n\n    u2 = data[FEATURES] \\\n            .head(10) \\\n            .to_numpy() \\\n            .astype(np.float32)\n\n    # Show\n    print(\"\\nData:\")\n    print(data)\n    print(\"\\nFeatures:\")\n    print(FEATURES)\n\n    # -------------------\n    # Create transformers\n    # -------------------\n    # Libraries\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.preprocessing import MinMaxScaler\n\n    # Transformers\n    mmx = MinMaxScaler().fit(data[FEATURES])\n    std = StandardScaler().fit(data[FEATURES])\n\n\n    # ----------------------------------------\n    # Create/Load model\n    # ----------------------------------------\n    # Libraries\n    from ls2d.autoencoder import AE\n    from ls2d.autoencoder import SkorchAE\n    from ls2d.pipeline import PipelineMemory\n\n    # Path\n    PATH = Path('../datasets/dengue/models')\n\n    # Load OLD model\n    m_old = _load_pickle(PATH / 'ae_sig_3')\n\n\n\n    # Autoencoder\n    # -----------\n    # Create\n    ae = AE(layers=[5, 3, 2])\n\n    # Show\n    print(\"\\nOriginal:\")\n    print(ae)\n\n    # Set encoder and decoder\n    ae.encoder = m_old._modules['encoder']\n    ae.decoder = m_old._modules['decoder']\n\n    # Sow\n    print(\"\\nEncoder & Decoder:\")\n    print(ae)\n\n    # Predictions\n    x1 = ae.encode_inputs(u1)\n    x2 = ae.transform(u1)\n    x3 = ae.transform(u2)\n\n    # Show\n    print(\"\\nAE predictions:\")\n    print(x1)\n    print(x2)\n    print(x3)\n\n    # Skorch\n    # ------\n    # Create\n    sae = SkorchAE(\n        module=AE,\n        module__layers=[5,3,2],\n        criterion=torch.nn.MSELoss)\n\n    # Initialize\n    sae = sae.initialize()\n\n    # Predictions\n    x4 = sae.transform(u1)\n    x5 = sae.transform(u2)\n\n    # .. note: Why sae predictions are different than those in\n    #          the AE model? They also vary for each execution\n    #          so probably not properly initialised. Skorch is\n    #          not needed if the model is created manually as\n    #          far as AE has transform and fit.\n\n    # Show\n    print(\"\\nSAE predictions:\")\n    print(x4)\n    print(x5)\n\n    # Pipeline\n    # --------\n    # Create pipeline\n    pipe = PipelineMemory(steps=[\n        ('minmax', mmx),\n        ('ae', ae)\n    ])\n\n    # Predictions\n    x6 = pipe.transform(u2)\n\n    # Show\n    print(\"\\nPipeline predictions:\")\n    print(x6)\n\n    # ------------------------\n    # Save all\n    # ------------------------\n    # Define pipeline path\n    uuid = time.strftime(\"%Y%m%d-%H%M%S\")\n    path = Path('./objects') / uuid\n\n    # Create folder\n    path.mkdir(parents=True, exist_ok=True)\n\n    # Save\n    _dump_pickle(path / 'pipe.p', pipe)\n    _dump_pickle(path / 'mmx.p', mmx)\n    _dump_pickle(path / 'std.p', std)\n\n    # ------------------------\n    # Double check\n    # ------------------------\n    # Libraries\n\n    # Format data\n    aux = data.copy(deep=True)\n    aux = aux[config['aggregations'].keys()] \\\n        .groupby(by='study_no', dropna=False) \\\n        .agg(config['aggregations'])\n\n    # Add projections\n    aux[['x', 'y']] = pipe.transform(aux[FEATURES])\n    data[['x', 'y']] = pipe.transform(data[FEATURES])\n\n    # Display\n    plt.scatter(data.x, data.y, s=2)\n    plt.figure()\n    plt.scatter(aux.x, aux.y, s=2)\n    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}